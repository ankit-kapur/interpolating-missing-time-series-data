{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the datasets\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "from numpy import concatenate, array\n",
    "from numpy.random import randn\n",
    "# Decimal precision value to display in the matrix\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "# Scipy\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "#mpl.rc('figure', figsize=(10, 8))\n",
    "\n",
    "# DBscan from sklearn\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pandas experiments\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame, Panel\n",
    "\n",
    "# Misc\n",
    "import time\n",
    "import datetime as dt\n",
    "import math\n",
    "import random\n",
    "print 'All libraries loaded.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataframes from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make dataframes from data\n",
    "\n",
    "# eGFR data\n",
    "egfr_df = pd.read_csv('~/code/independent/datasets/cdr_gfr_derived.csv', parse_dates=['resultdata'])\n",
    "egfr_df.drop('gfr', axis=1, inplace=True)\n",
    "egfr_df.columns = ['pid', 'timestamp', 'gender', 'birthyear', 'age', 'gfr']\n",
    "\n",
    "# Findings data\n",
    "findings_df = pd.read_csv('~/code/independent/datasets/cdr_finding.csv', parse_dates=['finddate'], usecols=['idperson', 'finddate', 'valuename', 'findvalnum'])\n",
    "findings_df = findings_df[['idperson', 'finddate', 'valuename', 'findvalnum']]\n",
    "findings_df.columns = ['pid', 'timestamp', 'testname', 'testval']\n",
    "\n",
    "# Lab reports data\n",
    "lab_df = pd.read_csv('~/code/independent/datasets/cdr_lab_result.csv', parse_dates=['resultdate'], usecols=['idperson', 'resultdate', 'valuename', 'resultvaluenum'])\n",
    "lab_df = lab_df[['idperson', 'resultdate', 'valuename', 'resultvaluenum']]\n",
    "lab_df.columns = ['pid', 'timestamp', 'testname', 'testval']\n",
    "# Make all lab tests values uppercase\n",
    "lab_df.testname = map(lambda x: x.upper(), lab_df.testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize dates (to remove the time part of it)\n",
    "egfr_df.timestamp = egfr_df.timestamp.map(pd.datetools.normalize_date)\n",
    "findings_df.timestamp = findings_df.timestamp.map(pd.datetools.normalize_date)\n",
    "lab_df.timestamp = lab_df.timestamp.map(pd.datetools.normalize_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1% of the values are NaN\n"
     ]
    }
   ],
   "source": [
    "# NaN values\n",
    "total_rowcount = len(lab_df.testval.values)\n",
    "nan_rowcount = len([x for x in lab_df.testval.values if math.isnan(x)])\n",
    "print '\\n',(str(nan_rowcount*100/total_rowcount)+ \"% of the values are NaN\")\n",
    "\n",
    "# Drop NaN rows\n",
    "# lab_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make tiny versions of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# howmany = 500000\n",
    "\n",
    "# egfr_df = egfr_df[:howmany]\n",
    "# lab_df = lab_df[:howmany]\n",
    "# findings_df = findings_df[:howmany]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a multi-index/hierarchical index \n",
    "#### The index will be a combination of the PID (person ID) and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the index as a combination of the person ID and timestamp\n",
    "egfr_df.set_index(['pid', 'timestamp'], inplace=True)\n",
    "findings_df.set_index(['pid', 'timestamp'], inplace=True)\n",
    "lab_df.set_index(['pid', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all patients for whom we have eGFR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 63215 unique patients\n"
     ]
    }
   ],
   "source": [
    "list_of_patients = list(set(egfr_df.index.get_level_values('pid').values))\n",
    "print '\\nFound', len(list_of_patients), 'unique patients'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get small sample of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get just 5 patients\n",
    "# list_of_patients = random.sample(list_of_patients, 5)\n",
    "\n",
    "# list_of_patients = [8555317, 8555928, 8565179]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the new dataframe (empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Findings: ['FND_BPS', 'FND_BPD']\n",
      "Lab tests: ['LR_AST', 'LR_MICROCR', 'LR_HDL', 'LR_TRIG', 'LR_A1C', 'LR_CR', 'LR_PTH', 'LR_GLUCNONFAST', 'LR_LDL', 'LR_GFR', 'LR_VITD 25', 'LR_ALT', 'LR_PHOS', 'LR_GFR_AFRAMER']\n"
     ]
    }
   ],
   "source": [
    "# Column names will be a combination of all the lab test names and finding names\n",
    "unique_findings = set(findings_df.testname.values)\n",
    "unique_labtests = set(lab_df.testname.values)\n",
    "print '\\nFindings:', list(unique_findings)\n",
    "print 'Lab tests:', list(unique_labtests)\n",
    "final_col_names = np.append(list(unique_findings), list(unique_labtests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Row locations in the eGFR dataframe of patients\n",
    "# locations_of_patients = []\n",
    "# for patient in list_of_patients:\n",
    "#     get_loc_result = egfr_df.index.get_loc(patient)\n",
    "#     locations_of_patients += [i for i,v in enumerate(get_loc_result) if v == True]\n",
    "\n",
    "# combined_df = DataFrame(egfr_df.iloc[locations_of_patients])\n",
    "combined_df = pd.DataFrame(egfr_df)\n",
    "\n",
    "# Add the new columns\n",
    "for newcol in final_col_names:\n",
    "    combined_df[newcol] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill values into the new dataframe<br>based on joins on the other dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fill for 0 patients completed in 0.000 seconds\n",
      "Data fill for 500 patients completed in 0.014 seconds\n",
      "Data fill for 1000 patients completed in 0.014 seconds\n",
      "Data fill for 1500 patients completed in 0.014 seconds\n",
      "Data fill for 2000 patients completed in 0.014 seconds\n",
      "Data fill for 2500 patients completed in 0.017 seconds\n",
      "Data fill for 3000 patients completed in 0.019 seconds\n",
      "Data fill for 3500 patients completed in 0.014 seconds\n",
      "Data fill for 4000 patients completed in 0.013 seconds\n",
      "Data fill for 4500 patients completed in 0.022 seconds\n",
      "Data fill for 5000 patients completed in 0.015 seconds\n",
      "Data fill for 5500 patients completed in 0.013 seconds\n",
      "Data fill for 6000 patients completed in 0.013 seconds\n",
      "Data fill for 6500 patients completed in 0.013 seconds\n",
      "Data fill for 7000 patients completed in 0.012 seconds\n",
      "Data fill for 7500 patients completed in 0.013 seconds\n",
      "Data fill for 8000 patients completed in 0.012 seconds\n",
      "Data fill for 8500 patients completed in 0.011 seconds\n",
      "Data fill for 9000 patients completed in 0.018 seconds\n",
      "Data fill for 9500 patients completed in 0.016 seconds\n",
      "Data fill for 10000 patients completed in 0.012 seconds\n",
      "Data fill for 10500 patients completed in 0.011 seconds\n",
      "Data fill for 11000 patients completed in 16.063 seconds\n",
      "Data fill for 11500 patients completed in 149.277 seconds\n",
      "Data fill for 12000 patients completed in 160.109 seconds\n",
      "Data fill for 12500 patients completed in 169.296 seconds\n",
      "Data fill for 13000 patients completed in 232.218 seconds\n",
      "Data fill for 13500 patients completed in 145.073 seconds\n",
      "Data fill for 14000 patients completed in 163.930 seconds\n",
      "Data fill for 14500 patients completed in 135.403 seconds\n",
      "Data fill for 15000 patients completed in 143.918 seconds\n",
      "Data fill for 15500 patients completed in 133.462 seconds\n",
      "Data fill for 16000 patients completed in 121.046 seconds\n",
      "Data fill for 16500 patients completed in 126.001 seconds\n",
      "Data fill for 17000 patients completed in 127.864 seconds\n",
      "Data fill for 17500 patients completed in 123.901 seconds\n",
      "Data fill for 18000 patients completed in 117.377 seconds\n",
      "Data fill for 18500 patients completed in 111.786 seconds\n",
      "Data fill for 19000 patients completed in 99.191 seconds\n",
      "Data fill for 19500 patients completed in 82.037 seconds\n",
      "Data fill for 20000 patients completed in 140.092 seconds\n",
      "Data fill for 20500 patients completed in 91.051 seconds\n",
      "Data fill for 21000 patients completed in 88.727 seconds\n",
      "Data fill for 21500 patients completed in 82.673 seconds\n",
      "Data fill for 22000 patients completed in 87.116 seconds\n",
      "Data fill for 22500 patients completed in 76.446 seconds\n",
      "Data fill for 23000 patients completed in 68.509 seconds\n",
      "Data fill for 23500 patients completed in 64.440 seconds\n",
      "Data fill for 24000 patients completed in 97.407 seconds\n",
      "Data fill for 24500 patients completed in 54.493 seconds\n",
      "Data fill for 25000 patients completed in 48.383 seconds\n",
      "Data fill for 25500 patients completed in 40.741 seconds\n",
      "Data fill for 26000 patients completed in 39.673 seconds\n",
      "Data fill for 26500 patients completed in 29.607 seconds\n",
      "Data fill for 27000 patients completed in 26.199 seconds\n",
      "Data fill for 27500 patients completed in 25.271 seconds\n",
      "Data fill for 28000 patients completed in 0.012 seconds\n",
      "Data fill for 28500 patients completed in 0.013 seconds\n",
      "Data fill for 29000 patients completed in 0.013 seconds\n",
      "Data fill for 29500 patients completed in 0.015 seconds\n",
      "Data fill for 30000 patients completed in 0.013 seconds\n",
      "Data fill for 30500 patients completed in 0.012 seconds\n",
      "Data fill for 31000 patients completed in 0.015 seconds\n",
      "Data fill for 31500 patients completed in 0.011 seconds\n",
      "Data fill for 32000 patients completed in 0.013 seconds\n",
      "Data fill for 32500 patients completed in 0.012 seconds\n",
      "Data fill for 33000 patients completed in 0.015 seconds\n",
      "Data fill for 33500 patients completed in 0.011 seconds\n",
      "Data fill for 34000 patients completed in 0.011 seconds\n",
      "Data fill for 34500 patients completed in 0.013 seconds\n",
      "Data fill for 35000 patients completed in 0.013 seconds\n",
      "Data fill for 35500 patients completed in 0.015 seconds\n",
      "Data fill for 36000 patients completed in 0.011 seconds\n",
      "Data fill for 36500 patients completed in 0.013 seconds\n",
      "Data fill for 37000 patients completed in 0.014 seconds\n",
      "Data fill for 37500 patients completed in 0.014 seconds\n",
      "Data fill for 38000 patients completed in 0.011 seconds\n",
      "Data fill for 38500 patients completed in 0.013 seconds\n",
      "Data fill for 39000 patients completed in 0.013 seconds\n",
      "Data fill for 39500 patients completed in 0.014 seconds\n",
      "Data fill for 40000 patients completed in 0.012 seconds\n",
      "Data fill for 40500 patients completed in 0.013 seconds\n",
      "Data fill for 41000 patients completed in 0.012 seconds\n",
      "Data fill for 41500 patients completed in 0.013 seconds\n",
      "Data fill for 42000 patients completed in 0.012 seconds\n",
      "Data fill for 42500 patients completed in 0.011 seconds\n",
      "Data fill for 43000 patients completed in 0.013 seconds\n",
      "Data fill for 43500 patients completed in 0.012 seconds\n",
      "Data fill for 44000 patients completed in 0.012 seconds\n",
      "Data fill for 44500 patients completed in 0.012 seconds\n",
      "Data fill for 45000 patients completed in 0.012 seconds\n",
      "Data fill for 45500 patients completed in 0.013 seconds\n",
      "Data fill for 46000 patients completed in 0.012 seconds\n",
      "Data fill for 46500 patients completed in 0.012 seconds\n",
      "Data fill for 47000 patients completed in 0.011 seconds\n",
      "Data fill for 47500 patients completed in 0.011 seconds\n",
      "Data fill for 48000 patients completed in 0.017 seconds\n",
      "Data fill for 48500 patients completed in 0.016 seconds\n",
      "Data fill for 49000 patients completed in 0.012 seconds\n",
      "Data fill for 49500 patients completed in 0.011 seconds\n",
      "Data fill for 50000 patients completed in 0.016 seconds\n",
      "Data fill for 50500 patients completed in 0.012 seconds\n",
      "Data fill for 51000 patients completed in 0.014 seconds\n",
      "Data fill for 51500 patients completed in 0.011 seconds\n",
      "Data fill for 52000 patients completed in 0.013 seconds\n",
      "Data fill for 52500 patients completed in 0.012 seconds\n",
      "Data fill for 53000 patients completed in 0.016 seconds\n",
      "Data fill for 53500 patients completed in 0.014 seconds\n",
      "Data fill for 54000 patients completed in 0.014 seconds\n",
      "Data fill for 54500 patients completed in 0.013 seconds\n",
      "Data fill for 55000 patients completed in 0.014 seconds\n",
      "Data fill for 55500 patients completed in 0.011 seconds\n",
      "Data fill for 56000 patients completed in 0.013 seconds\n",
      "Data fill for 56500 patients completed in 0.013 seconds\n",
      "Data fill for 57000 patients completed in 0.014 seconds\n",
      "Data fill for 57500 patients completed in 0.011 seconds\n",
      "Data fill for 58000 patients completed in 0.013 seconds\n",
      "Data fill for 58500 patients completed in 0.013 seconds\n",
      "Data fill for 59000 patients completed in 0.014 seconds\n",
      "Data fill for 59500 patients completed in 0.011 seconds\n",
      "Data fill for 60000 patients completed in 0.013 seconds\n",
      "Data fill for 60500 patients completed in 0.013 seconds\n",
      "Data fill for 61000 patients completed in 0.014 seconds\n",
      "Data fill for 61500 patients completed in 0.011 seconds\n",
      "Data fill for 62000 patients completed in 0.013 seconds\n",
      "Data fill for 62500 patients completed in 0.012 seconds\n",
      "Data fill for 63000 patients completed in 0.012 seconds\n",
      "Task completed in 3419.996 seconds\n"
     ]
    }
   ],
   "source": [
    "def fill_data(source_df, patient_id):\n",
    "    \n",
    "    ############### Could optimize\n",
    "    locations_of_patients = []\n",
    "    if str(patient_id) in source_df.index:\n",
    "        \n",
    "#         get_loc_result = source_df.index.get_loc(str(patient_id))\n",
    "#         locations_of_patients += [i for i,v in enumerate(get_loc_result) if v == True]\n",
    "#         this_patients_data = source_df.iloc[locations_of_patients]\n",
    "        this_patients_data = source_df.loc[str(patient_id)]\n",
    "\n",
    "        # We only need data for the dates that exist in the combined DF already \n",
    "        # (the ones that have eGFR readings)\n",
    "        for timestamp in combined_df.loc[patient_id].index:\n",
    "            if timestamp in this_patients_data.index.get_level_values('timestamp'):\n",
    "                this_patient = this_patients_data.loc[timestamp]\n",
    "\n",
    "                # Set the value in the DF for this test\n",
    "                # Check if there's multiple values for the same timestamp\n",
    "                if type(this_patient) == pd.core.frame.DataFrame:\n",
    "                    for i in range(this_patient.testname.size):\n",
    "                        \n",
    "                        try:\n",
    "                            combined_df.loc[(patient_id, timestamp), this_patient.testname[i]] = this_patient.testval[i]\n",
    "                        except:\n",
    "                            print 'multivalues for', patient_id\n",
    "                else:\n",
    "                    combined_df.loc[(patient_id, timestamp), this_patient.testname] = this_patient.testval\n",
    "\n",
    "                    \n",
    "fill_count = 0\n",
    "start_time = time.time()\n",
    "lastseen_time = time.time()\n",
    "\n",
    "for patient_id in list_of_patients:\n",
    "    \n",
    "    if fill_count % 500 == 0:\n",
    "        elapsed_time = time.time() - lastseen_time\n",
    "        lastseen_time = time.time()\n",
    "        print 'Data fill for', fill_count, 'patients completed in', '{:.3f}'.format(elapsed_time), 'seconds'\n",
    "    fill_count += 1\n",
    "    \n",
    "#     ########################################## REMOVE\n",
    "#     if fill_count < 14001:\n",
    "#         continue\n",
    "    \n",
    "    # Check if the findings df has data about this patient\n",
    "    fill_data(findings_df, patient_id)\n",
    "\n",
    "    # Check if the lab-tests df has data about this patient\n",
    "    fill_data(lab_df, patient_id)\n",
    "    \n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print 'Task completed in', '{:.3f}'.format(elapsed_time), 'seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the resultant DF to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the code for writing to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "86% of the values are NaN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9322160"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many values are NaNs\n",
    "total_rowcount = len(combined_df.FND_BPS.values)\n",
    "nan_rowcount = len([x for x in combined_df.FND_BPS.values if math.isnan(x)])\n",
    "print '\\n',(str(nan_rowcount*100/total_rowcount)+ \"% of the values are NaN\")\n",
    "combined_df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the dataframe to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file_path = 'combined_df.csv'\n",
    "combined_df.to_csv(output_file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466108, 20)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egfr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466108, 20)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lab_df.loc['8555580', '2010-02-22']\n",
    "\n",
    "# lab_df.loc['8554722'].loc['2009-02-20']\n",
    "# lab_df.loc['8554722']\n",
    "\n",
    "lab_df.iloc[lab_df.index.get_level_values('pid')=='8554722']\n",
    "\n",
    "# x = lab_df.xs('8555580', level='pid')['2010-02-22']\n",
    "# x\n",
    "\n",
    "print lab_df.xs('8555580', level='pid')['2010-02-22']\n",
    "\n",
    "# lab_df\n",
    "# lab_df['8554722']['2008-04-07']\n",
    "# lab_df['2010-02-22']\n",
    "\n",
    "# .xs('2009-02-20', level='timestamp')\n",
    "# x.index.searchsorted(stamp)\n",
    "# x['2008-02-04']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
